# app/scripts/scrape_curriculum_books.py

"""
Script de web scraping pour r√©cup√©rer les listes de livres recommand√©s
pour diff√©rents cursus universitaires alg√©riens.

Usage:
    python -m app.scripts.scrape_curriculum_books
"""

import sys
import os
from pathlib import Path
import requests
from bs4 import BeautifulSoup
from typing import List, Dict
import time
from datetime import datetime

# Ajouter le r√©pertoire parent au path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from sqlalchemy.orm import Session
from app.database import SessionLocal
from app.models.curriculum import Curriculum, RecommendedBook

# Configuration
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
}

# URLs √† scraper (exemples - √† adapter selon les sources r√©elles)
SOURCES = [
    {
        "name": "L1 Informatique USTHB",
        "university": "USTHB",
        "field": "Informatique",
        "year": "1√®re ann√©e",
        "url": "https://www.usthb.dz/fei/programmes/l1-informatique",  # URL fictive
        "selector": ".book-list .book-item"  # S√©lecteur CSS fictif
    },
    {
        "name": "1√®re Ann√©e M√©decine Universit√© d'Alger",
        "university": "Universit√© d'Alger 1",
        "field": "M√©decine",
        "year": "1√®re ann√©e",
        "url": "https://www.univ-alger.dz/medecine/1ere-annee",  # URL fictive
        "selector": ".recommended-books .book"
    },
    {
        "name": "L1 Math√©matiques USTHB",
        "university": "USTHB",
        "field": "Math√©matiques",
        "year": "1√®re ann√©e",
        "url": "https://www.usthb.dz/fs/maths/l1",  # URL fictive
        "selector": ".course-books li"
    }
]


def scrape_usthb_informatique() -> List[Dict]:
    """
    Scraper sp√©cifique pour USTHB Informatique
    √Ä adapter selon la structure r√©elle du site
    """
    books = []
    
    # Exemple de donn√©es hardcod√©es (√† remplacer par du vrai scraping)
    sample_books = [
        {"title": "Algorithmique et structures de donn√©es", "author": "Thomas H. Cormen"},
        {"title": "Introduction √† Python", "author": "G√©rard Swinnen"},
        {"title": "Architecture des ordinateurs", "author": "Andrew S. Tanenbaum"},
        {"title": "Math√©matiques pour l'informatique", "author": "Donald Knuth"},
        {"title": "Syst√®mes d'exploitation", "author": "Abraham Silberschatz"}
    ]
    
    return sample_books


def scrape_medecine_alger() -> List[Dict]:
    """
    Scraper sp√©cifique pour M√©decine Alger
    """
    books = [
        {"title": "Anatomie humaine", "author": "Frank H. Netter"},
        {"title": "Physiologie m√©dicale", "author": "Guyton et Hall"},
        {"title": "Biochimie m√©dicale", "author": "Harper"},
        {"title": "Histologie", "author": "Ross et Pawlina"},
        {"title": "Embryologie humaine", "author": "Larsen"}
    ]
    
    return books


def scrape_maths_usthb() -> List[Dict]:
    """
    Scraper sp√©cifique pour Math√©matiques USTHB
    """
    books = [
        {"title": "Analyse math√©matique I", "author": "Vladimir Zorich"},
        {"title": "Alg√®bre lin√©aire", "author": "Serge Lang"},
        {"title": "Topologie g√©n√©rale", "author": "James Munkres"},
        {"title": "Probabilit√©s et statistiques", "author": "Sheldon Ross"},
        {"title": "Calcul diff√©rentiel", "author": "Michael Spivak"}
    ]
    
    return books


def scrape_generic(url: str, selector: str) -> List[Dict]:
    """
    Scraper g√©n√©rique pour n'importe quelle page
    
    Args:
        url: URL de la page √† scraper
        selector: S√©lecteur CSS pour trouver les livres
    
    Returns:
        Liste de dictionnaires avec titre et auteur
    """
    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        books = []
        
        book_elements = soup.select(selector)
        
        for element in book_elements:
            # Adapter selon la structure HTML r√©elle
            title = element.select_one('.title, h3, .book-title')
            author = element.select_one('.author, .book-author')
            
            if title:
                book_data = {
                    "title": title.get_text(strip=True),
                    "author": author.get_text(strip=True) if author else None
                }
                books.append(book_data)
        
        return books
        
    except Exception as e:
        print(f"‚ùå Erreur lors du scraping de {url}: {e}")
        return []


def save_curriculum_books(db: Session, curriculum_data: Dict, books: List[Dict]):
    """
    Sauvegarder le cursus et ses livres dans la base de donn√©es
    
    Args:
        db: Session de base de donn√©es
        curriculum_data: Donn√©es du cursus
        books: Liste des livres recommand√©s
    """
    try:
        # Cr√©er ou r√©cup√©rer le cursus
        curriculum = db.query(Curriculum).filter(
            Curriculum.name == curriculum_data["name"]
        ).first()
        
        if not curriculum:
            curriculum = Curriculum(
                name=curriculum_data["name"],
                university=curriculum_data["university"],
                field=curriculum_data["field"],
                year=curriculum_data["year"],
                source_url=curriculum_data.get("url")
            )
            db.add(curriculum)
            db.flush()
        
        print(f"‚úÖ Cursus: {curriculum.name}")
        
        # Ajouter les livres
        for book_data in books:
            # V√©rifier si le livre existe d√©j√†
            existing_book = db.query(RecommendedBook).filter(
                RecommendedBook.title == book_data["title"],
                RecommendedBook.author == book_data.get("author")
            ).first()
            
            if not existing_book:
                recommended_book = RecommendedBook(
                    title=book_data["title"],
                    author=book_data.get("author"),
                    isbn=book_data.get("isbn"),
                    source_url=curriculum_data.get("url")
                )
                db.add(recommended_book)
                db.flush()
            else:
                recommended_book = existing_book
            
            # Lier le livre au cursus
            if recommended_book not in curriculum.recommended_books:
                curriculum.recommended_books.append(recommended_book)
            
            print(f"  üìö {book_data['title']} - {book_data.get('author', 'Auteur inconnu')}")
        
        db.commit()
        print(f"‚úÖ {len(books)} livres ajout√©s pour {curriculum.name}\n")
        
    except Exception as e:
        print(f"‚ùå Erreur lors de la sauvegarde: {e}")
        db.rollback()


def run_scraping():
    """
    Fonction principale d'ex√©cution du scraping
    """
    print("\n" + "="*60)
    print("üï∑Ô∏è  SCRAPING DES LISTES DE LIVRES RECOMMAND√âS")
    print("="*60 + "\n")
    
    db = SessionLocal()
    
    try:
        # 1. USTHB Informatique
        print("üîç Scraping: L1 Informatique USTHB...")
        books = scrape_usthb_informatique()
        save_curriculum_books(db, {
            "name": "L1 Informatique USTHB",
            "university": "USTHB",
            "field": "Informatique",
            "year": "1√®re ann√©e"
        }, books)
        time.sleep(1)
        
        # 2. M√©decine Alger
        print("üîç Scraping: 1√®re Ann√©e M√©decine Universit√© d'Alger...")
        books = scrape_medecine_alger()
        save_curriculum_books(db, {
            "name": "1√®re Ann√©e M√©decine Universit√© d'Alger",
            "university": "Universit√© d'Alger 1",
            "field": "M√©decine",
            "year": "1√®re ann√©e"
        }, books)
        time.sleep(1)
        
        # 3. Math√©matiques USTHB
        print("üîç Scraping: L1 Math√©matiques USTHB...")
        books = scrape_maths_usthb()
        save_curriculum_books(db, {
            "name": "L1 Math√©matiques USTHB",
            "university": "USTHB",
            "field": "Math√©matiques",
            "year": "1√®re ann√©e"
        }, books)
        
        print("\n" + "="*60)
        print("‚úÖ SCRAPING TERMIN√â AVEC SUCC√àS")
        print("="*60 + "\n")
        
    except Exception as e:
        print(f"\n‚ùå Erreur globale: {e}")
        db.rollback()
        
    finally:
        db.close()


if __name__ == "__main__":
    run_scraping()
